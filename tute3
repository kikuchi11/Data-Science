1. Consider the following 10 instances, given so-called “gold standard” labels 
(assuming a 3-class problem), and the output of four supervised machine learning models:
Given four models

P = {A, B, C}
*all labels/classes are important so put them in positive
N = {?}
*missing val goes into negative

(a) Where possible, calculate the accuracy and error rate of the four models.

For system 1, there are 6 correct responses (instances 1, 3, 4, 5, 9, and 10 — where the prediction is the same as the “Gold” label), out of the 10 instances, so that the accuracy is 60%.
– For system3 , the 5 correct responses(alloftheAinstances),so that the accuracy is 50%.

The latter indicates the predicted ones
FP = negative, predicted positive
TP = positive, predicted positive
FN = negative, predicted negative
TN = positive, predicted negative

Theerrorrateofsystem1is1−0.6=0.4
Theerrorrateofsystem2is1−0.5=0.5
Theerrorrateofsystem3is1−0.5=0.5
Theerrorrateofsystem4is1−0.4=0.6

Accuracy rate = (TP + TN) /  (TP+FP+FN+TN)
Error rate  = (FP + FN) / (TP+FP+FN+TN)

*Accuracy rate -> Actually positive but predicted to be positive and negative

Model 2:
All classes are important but not the missing values.
FP = 10
FN = 0
TP = 10
TN = 0

*If there are more than one predicted values, such as A or B or C, then the FP(predicted to be
positive but negative) is can be multiple.

Model 4:
TP = 4
FP = 0
FN = 6

Acc = 4/10 = 40%
ERR = 1 - 0.4 = 0.6 


(b) Where possible, calculate the precision and recall, treating 
class A as the “positive” class. Do the same for the B and C classes, in turn, and then 
calculate the macro-averaged precision and recall.

For precision, with respect to one class/label only
Pre = TP / (TP + FP)

For class A
P = {A}
N = {A, B, C, ?}
TP = 4
FP = 3

A: Pre = 4/7

Sometimes there's pre = 0 / (0 + 0) -> say Not well defined.
The average of precision cannot be calculated in this case.

*Again with respect to one class only
Recall = TP / (TP+FN)
Macro averaged precision = average of precisions with respect to each label/class

If there's 0 / (0 + 0), it becomes not well defined.

2, What is the difference between evaluating using a holdout strategy 
and evaluating using a cross– validation strategy?

In a holdout evaluation strategy, we partition the data into a training set and a test set:
we build the model on the former, and evaluate on the latter.

In a cross-validation evaluation strategy, do the same as above, 
but a number of times, where each iteration uses one partition of the data as a test set
and the rest as a training set (and the partition is different each time).

(a) What are some reasons we would prefer one strategy over the other?

Holdout is subject to some random variation, depending on which
instances are assigned to the training data (because only do once), 
and which are assigned to the test data. 
Any instance that forms part of the model is excluded from testing, and vice versa. 
This could mean that our estimate of the performance of the model is way off, 
or changes a lot from data set to data set.

Cross–validation mostly solves this problem: we’re averaging over a 
bunch of values, so that one weird partition of the data 
won’t throw our estimate of performance completely off; also, 
each instance is used for testing, but also appears in the 
training data for the models built on the other partitions. 
It usually takes much longer to cross-validate, how- ever, 
because we need to train a model for every test partition.


3. For the following dataset:
(a) Classify the test instances using the method of 0-R.

0-R -> goes to the majority, ignore attributes.
So choose Y all the time as the training instances of play has more Ys than Ns.

(b) Classify the test instances using the method of 1-R.

1-R -> choose one attribute and formulate functions based on the majority.
o = s -> P = N (majority when o = s)
o = o -> p = y (majority when o = o)
o = r -> p = y

error = 1

Do the same for all attributes and see the number of errors, and pick the attribute with the
least number of errors.

*nobody uses zero-r, one-r in real life.

(c) Classify the test instances using the ID3 Decision Tree method:

      Outlook
      s o r
Y     0 1 2
N     2 0 1
Total 2 1 3
-----------------------------------------------------------------
P(Y)  0 1 2/3 # probability of Y
P(N)  1 0 1/3 # probability of N
-----------------------------------------------------------------
H     0 0 0.98 -> -(2/3log(2/3)...)
*MI is the aggregated value of entropies divided by each of its proportion.
MI    (2/6)(0) + 1/6(0) + 3/6 (0.918) = 0.45902 # 2/6... part comes from the total
IG    1 - 0.459 = 0.5408 *1 - Mutual Information is Information gain

Do this for other attributes and choose the best attribute with the bet information gain and gain 
ratio.

ID has 1 information gain but it has no information so should ignore it.

Using the Gain Ratio as a splitting criterion
“split information” — the entropy of the distribution of instances across the daughters of
a given attribute.

Why use gain ratio
 "For example, suppose that we are building a decision tree for some data describing a business's customers. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's credit card number. This attribute has a high information gain, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before."

