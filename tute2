1. Build a probabilistic model based around the given training instances:
(a) Calculate the prior probability P(Outl = s). Calculate the prior probabilities of the other attribute values in this data.
Prior Prob:
Prior Prob * Test Evid = Posterior Prob

Posterior Prob:
P(c | pos)  = P(c) * P(pos | c)

-> "In what sproportion of the instances is this true" (Maximum likelihood estimation of the prob)
e.g. P(Outl = s) = 3/6

(b) Find the entropy of (the distribution of the attribute values) for each of the six attributes, given this probabilistic model.
-> H(X) = − Sigma P(x)log2 P(x)

H(Outl) = −[p(s) log2 p(s) + p(o) log2 p(o) + p(r) log2 p(r)]
        = 1.46 bits

(c) Calculate the joint probability P (Outl = s ∩ Temp = h). Calculate some other joint probabil- ities, for pairs of attribute values from different attributes.
Joint probability -> Multiple events to happen at the same time.
-> “what proportion of instances has both of these as true?”

-> Count the number of instances that have the attribute values for Outl, 
and h for Temp: there are 2 (A and B), so the required proability
P(Outl=s∩Temp=h)is 2/6.

(d) Calculate the posterior (or conditional) probability P (Outl = s|Temp = h). Calculate some other posterior probabilities.
-> "what proportion of instances is the left-hand event true 
given that the right-hand condition is true?”

Given Temp is h, 
There are 3 such instances (A,B,andC) Of just these three instances, how many have 
Outl as s? 2 (A and B), so the required probability is 2 .

2. Ensure that you can derive the Naive Bayes formulation.

3. Using the probabilistic model that you developed above, classify the test instances according to
the method of Naive Bayes.
(a) Using the “epsilon” smoothing method.
To build a Naive Bayes classifier, need to calculate all of the prior probabilities of the classes, 
and all of the posterior probabilities of an attribute (value) given a class.

3 of the 6 instances are Y, and 3 of the 6 instances are N, so P(Y)= 3, and the same for 6
P(N).

For the posterior probabilities, read off the instances of that class, and count
the proportion which had that attribute value. For example, P (Outl=s|N) = 2/3 ; P (Outl=o|Y) = 1/3. 

We classify a test instance T by calculating “probabilities” for each class, as follows: (They aren’t truly probabilities.)

N : P(N)P(Outl=o|N)P(Temp=m|N)P(Humi=n|N)P(Wind=T|N)
= 3/6 × 0/3 × 0/3 × 2/3 × 2/3

Y : P(Y)P(Outl=o|Y)P(Temp=c|Y)P(Humi=n|Y)P(Wind=T|Y) = 3/6×1/3×1/3×1/3×0/3

(b) Using “Laplace” smoothing.








*Notes

In simple terms, a Naive Bayes classifier assumes that the presence of 
a particular feature in a class is unrelated to the presence of any other feature. 
For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. 
Even if these features depend on each other or upon the existence of the other features,
all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.

Pros:

It is easy and fast to predict class of test data set. It also perform well in multi class prediction
When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.
It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).
Cons:

If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.
On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.
Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.


