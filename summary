Week1

Supervised (labeled) vs Unsupervised (unlabled)

Supervised Learner
Train Data -> Learner

Test Data & Learner -> Model

Test Data & Model -> Evaluation

Evaluation (Supervised):

Pick an evaluation metric comparing label vs prediction

Metrics:
Accuracy, Contingency Table (Correlation), Precision-Recall (# of True Positives), ROC curves (curve of probability)

Accuracy = True Positive/Actual Results or True Positive/(True Positive + False Positive)
True Positive & False Positive: when the actual result and predicted value match


When data is poor, cross-validate

Knowledge Technologies (matching data, databases) vs Machine Learning (predicting based on models, AI)

-------------------------------------------------------------------------

3 models
Linear

P(y|x) = probability of y given x

P(x,y) = probability of having (Knowledge Tech = x, ML = y)

Probability

Fi = null

-------------------------------------------------------------------------

Axioms:
p(f) >= for every even f in F

p(omega) = 1

-------------------------------------------------------------------------

Discrete vs Continuous:

Discrete (Described by probability mass function):

Bernoulli : success or failure based on probability density function

Continuous (Described by probability density function):

Uniform (Rectangular one)

Normal (Probability distribution over all the real numbers, described by a mean and a variance) 

Laplace (difference between two independent variates)

Dirichlet (Sampling from the space of real numbers. Instead it is sampling over a probability simplex)
Probability Simplex : a bunch of numbers that add up to 1


-------------------------------------------------------------------------

Expectation E[X] is the X's average value.

Discrete: E[x] = Sigma x P(X=x) -> sum of the plobabilities where the expectation matches the variables
Continuous: E[x] = integral x p(x) dx -> since it's continuous, takes the area that the variable creates.

Properties:
Expectation is only effective on variables with X.
Linear:E[aX + b] = aE[X] + b
       E[X + Y] = E[X] + E[Y]

If X is greater than Y, then the expectations will have the same innequality relationship
Monotone: X >= Y -> E[X] >= E[Y]

Variance: Var(X) = E[(X - E[X])^2]


-------------------------------------------------------------------------

Independence and Conditioning

-> = is a subset of 

X, Y are independent if 
P(X -> A, Y -> B) = P(X -> A) P(Y -> B)
Same for densities.

Intuitively: knowing value of Y reveals nothing about X

Algebraically: the joint on X,Y factorises!

Cond Prob
P(A|B) = P(A n B) / P(B)
Same for densities.

-------------------------------------------------------------------------

Inverting Conditioning: Bayes' Theorem

P(AnB) = P(A|B) P(B) = P(B|A) P(A)
(A given B) times B -> There's B known and A, times B implies given B doesn't matter.

P(A|B) = (p(B|A) p(A))/P(B)

Bayesian Stat Inference
Marginals: prob of individual vars
Marginalisation: summing away all

-------------------------------------------------------------------------

Instance is a row, and attribute is a column

Classification -> supervised. Set of classified training data.  Measure success on test data.

Clustering -> unsupervised. Without training data.  Success measured subjectively.
Grouping similar items.  Dynamically discover the classes (strong).  Categorise instances as certain lables without preclassified data (weak)

Regression -> Classification learning, but class is continuous (numeric prediction)
supervised. In classification, always map an instance to the correct label.  In regression, not sure if it's correct
but accept close values.

Association -> detect useful patterns among sets of items, potentially many association rules.

-------------------------------------------------------------------------

Nominal -> {sunny,overcast,rainy} serve only as labels or names, categorical, discrete, only equality tests
, nominal and ordinal are not always clear in distinction

Ordinal -> explicit order is imposed on the values, hot > mild > cool

Continuous -> maths allowed, attribute distance.

-------------------------------------------------------------------------



