Week1

Supervised (labeled) vs Unsupervised (unlabled)

Supervised Learner
Train Data -> Learner

Test Data & Learner -> Model

Test Data & Model -> Evaluation

Evaluation (Supervised):

Pick an evaluation metric comparing label vs prediction

Metrics:
Accuracy, Contingency Table (Correlation), Precision-Recall (# of True Positives), ROC curves (curve of probability)

Accuracy = True Positive/Actual Results or True Positive/(True Positive + False Positive)
True Positive & False Positive: when the actual result and predicted value match


When data is poor, cross-validate

Knowledge Technologies (matching data, databases) vs Machine Learning (predicting based on models, AI)

-------------------------------------------------------------------------

3 models
Linear

P(y|x) = probability of y given x

P(x,y) = probability of having (Knowledge Tech = x, ML = y)

Probability

Fi = null

-------------------------------------------------------------------------

Axioms:
p(f) >= for every even f in F

p(omega) = 1

-------------------------------------------------------------------------

Discrete vs Continuous:

Discrete (Described by probability mass function):

Bernoulli : success or failure based on probability density function

Continuous (Described by probability density function):

Uniform (Rectangular one)

Normal (Probability distribution over all the real numbers, described by a mean and a variance) 

Laplace (difference between two independent variates)

Dirichlet (Sampling from the space of real numbers. Instead it is sampling over a probability simplex)
Probability Simplex : a bunch of numbers that add up to 1


-------------------------------------------------------------------------

Expectation E[X] is the X's average value.

Discrete: E[x] = Sigma x P(X=x) -> sum of the plobabilities where the expectation matches the variables
Continuous: E[x] = integral x p(x) dx -> since it's continuous, takes the area that the variable creates.

Properties:
Expectation is only effective on variables with X.
Linear:E[aX + b] = aE[X] + b
       E[X + Y] = E[X] + E[Y]

If X is greater than Y, then the expectations will have the same innequality relationship
Monotone: X >= Y -> E[X] >= E[Y]

Variance: Var(X) = E[(X - E[X])^2]


-------------------------------------------------------------------------

Independence and Conditioning

-> = is a subset of 

X, Y are independent if 
P(X -> A, Y -> B) = P(X -> A) P(Y -> B)
Same for densities.

Intuitively: knowing value of Y reveals nothing about X

Algebraically: the joint on X,Y factorises!

Cond Prob
P(A|B) = P(A n B) / P(B)
Same for densities.

-------------------------------------------------------------------------

Inverting Conditioning: Bayes' Theorem

P(AnB) = P(A|B) P(B) = P(B|A) P(A)
(A given B) times B -> There's B known and A, times B implies given B doesn't matter.

P(A|B) = (p(B|A) p(A))/P(B)

Bayesian Stat Inference
Marginals: prob of individual vars
Marginalisation: summing away all

-------------------------------------------------------------------------

Instance is a row, and attribute is a column

Classification -> supervised. Set of classified training data.  Measure success on test data.

Clustering -> unsupervised. Without training data.  Success measured subjectively.
Grouping similar items.  Dynamically discover the classes (strong).  Categorise instances as certain lables without preclassified data (weak)

Regression -> Classification learning, but class is continuous (numeric prediction)
supervised. In classification, always map an instance to the correct label.  In regression, not sure if it's correct
but accept close values.

Association -> detect useful patterns among sets of items, potentially many association rules.

-------------------------------------------------------------------------

Nominal -> {sunny,overcast,rainy} serve only as labels or names, categorical, discrete, only equality tests
, nominal and ordinal are not always clear in distinction

Ordinal -> explicit order is imposed on the values, hot > mild > cool

Continuous -> maths allowed, attribute distance.

-------------------------------------------------------------------------

Binomial Dist (yes or no, two outcomes):
B(m; n, p) = (n/m) p^m (1 − p)^(n−m)
=n! / m!((n − m)!)p^m(1 − p)

Intuition: we want exactly m successes (p ^ m) and n - m failures

Multinomai Dist:
More than two outcome but the same as binomial

Entropy (predictability)

A high entropy value means x is unpredictable

-------------------------------------------------------------------------

A model is our attempt to understand and represent the nature
of reality through a particular lens, be it architectural, biological,
or mathematical.

A model is an artificial construction where all extraneous detail
has been removed or abstracted.

to produce
• A mathematical function that approximates a set of
observations (and can be estimated from data).

There is a trade-off in modelling between simplicity and
accuracy.

A probability model predicts the relative frequency of each event
if the experiment is performed a large number of times. It
captures a theoretical probability

-------------------------------------------------------------------------

Probablistic Learner

Build a model of the training data, and use that to predict the class labels of the test data.

the result goes to the majority

Conditional independence assumption: each probability is independent but false in almost every dataset.

Multiply all cond prob and prob itself to get naive bayes.
e.g. Is Bob more likely to have a cold
Cold * Headache severe given cold * ....

P(C) * P(H=s|C)
In case there are 0s, maintain a prob dist 
Replace zero with a trivially small non-zero, epsilon.

Less than 1/n for n instances.
Assume (1+e) ,=, 1

Unseen events (events that are not taken into account) get a count of 1

1 becomes 2, 2 becomes 3
If a few instaces, it's gonna change the result but fine with big data.

-> Laplace smoothing or add-one smoothing (most common, reasonably accurate)
-> overestimate the liklihood of unseen events

Add-k smoothing: like laplace but instead of adding 1 to all counts, add k < 1

Good-Turing estimation: uses number of sigletons to estimate numer of unseen events; counts are progressibely adjusted

Regression: leads to a more complicated learner

-------------------------------------------------------------------------

Why does Baives Bayes work
-> don't need a correct estimate of P(C|T): only need to know which cj is the greatest

-> Robut to two common errors:
       overestimated some, but some underestimated as well
       marginally relevant atts are correlated.
       
-> works well with large datasets
-> many application areas
-> easy to explain

Processing dict, slow way sum the entries in the dict

Fastway: read off the class array

Things to do: if value is 0, replace with epsilion

-------------------------------------------------------------------------

Classification:
-> Supervised
Input: set of labelled training instances; set of unlabelled test instnces

Output: prediction

Good classifier
-> good accuracy -> number of correctly labelled test instances/ total of test instances

Use all of the instanes as training data
       - Build the classifier using all of the instances
       - use all the same instances as test data

Testing on the training data is not good

Each instance is randomly assigned as either a training instance or a testing instance.

Effectively partitioned, no overlap between datasets.

Typically: 50-50, 80-20, 90-10

Advantages:
Simple tow work with and implement
High reproducibility

Disadv:
       Lots of test instances: learner doesn't have enough info
       lot s of training instances: test data might not be representative.
       
Repeated random subsampling: randomly chosen each time, size of training set is fixed across iterations

Evaluate by averaging across the iterations.

Adv:
       more reliable
Disadv:
       More difficult to reproduce
       slower than holdout
       Wrong choice of training set size can lead to misleading res
       
Cross Valiation:
       Progressively split into a number of partitions m >= 2
       
       iteratively
              one partition is used as test data
              the other m -1 partitions are used as training data
       Eval:
              Averaging

Similar to testing on the training data, but as partition no dataset overlap
Same amout of time as repeated roughly
reproducible
minimise bias and variance of the classifier's perfo

m size?
Folds: impacts runtime and datasets
Feler folds: more instances per partition, more variance in perfo

More folds: fewer instances per partition, less variance but slower

Commonly: m = 10
Far too slow to use in practice


-------------------------------------------------------------------------

Inductive learning hypo:
Hypothesis found to approximate the target function well over training set will also appromimate the 
well over unseen test examples


MLers suffer from inductive bias:
       - different assumptions lead to diff predic
       - can only sensibly criticise assumptions with respect to actual data (empirical problem)
The only way to minimise bias is to know what the unseen dataset will look like.

Assume an interesting class and an uninteresting class:
Classifier
       - Insteresting True positive
       - Interesting False Negative
       - Uninteresting False Positive
       - Uninteresting True Negative
       
Precision: how often correct when predicting an instance is interesting
TP / (TP + FP)

Recall: What proportion of the truly interesting instances we correctly identified as interesting
TP / (TP + FN)

FN is something that is mistakenly labelled as negative

-------------------------------------------------------------------------

Precision/Recall -> inverse relo(
Classifer -> high precision but low recall
Classifier -> high recall but low precision

We want both to be high:
F-score to evaluate this

F = ((1 + B^2) PR)/ ((B^2) P) + R))
B= 1
F = 2PR/ (P+R)

High recall, high precision would look similar to accuracy

Precision/Recall/F-score are all calculated per-class, and must be averaged across c classes:

Micro averaging 

Precision = summation of class precision / # of class
Recall = summation of recall / # of class

Diff occur when don't know is permitted as a prediction:

Small ones can occur depending on when averagin takes place

-------------------------------------------------------------------------

Decision Tree

Baseline -> expect any reasonabbly well-developed method to better, at least acceptable.
e.g. for a novice marathoner, the time to walk is 42km

Benchmark -> established rival technique which we are pitching our method against
e.g. for a marathon runner, the time of our last marathon run/ the world record time/ 3 hours

Baseline is used for both meanings.

Baselines are important in establishing whether any proposed method is doing better than dumb and simple
       dumb methods often work surprisingly well
       
Baselines are valuable in getting a sense for the intrinsic difficulty of a given task (accuracy = 5% vs 99%))

Random Baseline:
Method 1: randomly assign a class to each test instance
       - Often the only option in unsupervised/semi-supervised contexts

Method 2: randomly assign a class ck to each test instance, weighting the class assignment according to P(ck)

       Assumes we know the class prior probabilities
       Alleviate effects of variance by:
              running method N times and calculating the mean accuracy
                     or
              arriving at a deterministic estimate of the accuracy of random assignment
              
Zero-R

method: classify all instances according to the most common class in the training data

The most commonly used baseline in machine learning
Known as majority class baseline
Inappropriate if the majority is false and the learning task is to identify needles in the hystack.

One-R One rule
Creates one rule for each attribute in the training data, then selects the rule with the smallest error rate as its one rule
Method: create a decision stump for each attribute, with branches for each value, and populate the leaf with the majority class 
at that leaf; select the decision stump which leads to the lowest error rate over the training data.

One-R Pseudo
For each att,
       for each value of the attribute, make a rule:
              count how often each class appears
              find the most frequent class
              make the rule assign that class to this value
calculate the error rate of the rules
Choose the attribute whose rules produce the smallest error rate.


Reflections:

Adv:
        simple to undestand and implement
        good results
Disadv:
       unable to capture attribute interactions
       bias towards high-arity attributes (attributes with many possible values)
       
       
-------------------------------------------------------------------------

Construct decision trees:

Basic method: construct decision trees in recursive
divide-and-conquer fashion

Having constructed the decision tree, we classify novel instances
by traversing down the tree and classifying according to the label
at the deepest reachable point in the tree structure (leaf).

Complications:
       unobserved attribute-value pairs
       missing vaue
       
Decision Trees can be read as a disjunction; for example, Yes:
(outlook = sunny ∧ humidity = normal)
∨(outlook = overcast)
∨(outlook = rainy ∧ windy = false)       

We want to get the smallest tree (Occam’s Razor;
generalisability). Prefer the shortest hypothesis that fits the data.
In favor:
• Fewer short hypotheses than long hypotheses
• a short hyp. that fits the data unlikely to be a coincidence
• a long hyp. that fits data might be a coincidence
Against:
• Many ways to define small sets of hypotheses


-------------------------------------------------------------------------

Information Gain

• The expected reduction in entropy caused by knowing the
value of an attribute.
• Compare:
• the entropy before splitting the tree using the attribute’s
values
• the weighted average of the entropy over the children after
the split (Mean Information)
• If the entropy decreases, then we have a better tree (more
predictable)


-------------------------------------------------------------------------

Mean Info(x1, .., xm) = Sigma
P(xi)H(xi)

pribability times entropy

IG(RA|R) = H(R) − sigma P(xi)H(xi)

Prefer higher branch attributes

Information gain tends to prefer highly-branching attributes:

• A subset of instances is more likely to be homogeneous (all
of a single class) if there are only a few instanes
• Attribute with many values will have fewer instances at
each child node

Solution: Gain Ratio

Gain ratio (GR) reduces the bias for information gain
towards highly-branching attributes by normalising relative
to the split information

Split info (SI) is the entropy of a given split (evenness of
the distribution of instances to attribute values)

-> being higher doesn't matter

• Split Info sometimes called Intrinsic Value
• Discourages the selection of attributes with many uniformly
distributed values

We recurse until the instances at a node are of the same
class

This is consistent with our usage of entropy: if all of the
instances are of a single class, the entropy of the
distribution is 0

Considering other attributes cannot “improve” an entropy of
0 — the Info Gain is 0 by definition

The Info Gain/Gain Ratio allows us to choose the
(seemingly) best attribute at a given node

However, it is also an approximate indication of how much
absolute improvement we expect from partitioning the data
according to the values of a given attribute

An Info Gain of 0 means that there is no improvement; a
very small improvement is often unjustifiable

Typical modification of ID3: choose best attribute only if
IG/GR is greater than some threshold τ

Other similar approaches use pruning — post-process the
tree to remove undesirable branches (with few instances, or
small IG/GR improvements)

Fall back to majority class label for instances at a leaf with
a mixed distribution — unclear what to do with ties

Possibly can be taken as evidence that the given attributes
are insufficient for solving the problem

ID3 Decision Trees
       - highly regarded, fast to train, even faster to classify
       - susceptible to the effects of irrelevant feature
       - some quirks to account for missing/continuous feature values.
       
ID3 is an indectuive learning algo.
Decision tree variant:

Oblivious Decision Trees: require the same attribute at every node in a layer.
Random tree: only uses a sample of the possible attributes at a given node
       - helps to account for irrelevant atts
       - basis for a b

The algorithm operates over a set of training instances, C.

If all instances in C are in class P, create a node P and stop, 
otherwise select a feature or attribute F and create a decision node.

Partition the training instances in C into subsets according to the values of V.

Apply the algorithm recursively to each of the subsets C.

The order in which attributes are chosen determines how complicated the tree is.
ID3 uses information theory to determine the most informative attribute.
A measure of the information content of a message is the inverse of the probability of receiving the message:

What are the theoretical and practical properties of ID3-style decision trees?
-financial assessments


-------------------------------------------------------------------------

Instances: the individual, independent examples of a concept: exemplars
Is described by n attribute value pairs
Each also has a class label.

Instance-based learning (IBL)
IBL algos -> supervised; learn from labelled examples.
Learn by example.
Mode: function that maps instances to categoris.

Similarity assessments
-> set intersection
Amazon: Book purchases
Netflix: Movies that you have watched

Rating sets (starts)
       - thresholding using ratings
       - different subsets for different ratings
Categoris of items
       - generalisation (child-parent)
       - book or movie genres.
       
       
Jaccard Similarity
sim(A,B) = |A n B| / |A u B|

Dice Coefficient
2 |A n B| / (|A| + |B|)

A + B doesn't exclude the area binded by both A n B

-> emphasis on the purely common vals

Feature vectors:
-> n dimensional vector of features that represent some object.
-> feature or attribute is any distinct aspect, quality, or characteristic of that object.

May be nominal / categorical, orginal, numeric

-------------------------------------------------------------------------

Similarity
• Numerical measure of how alike two data objects are.
• Is higher when objects are more alike.
• Often falls in the range [0,1]

Dissimilarity
• Numerical measure of how different are two data objects
• Lower when objects are more alike
• Minimum dissimilarity is often 0
• Upper limit varies

Distance measures:
measure on a space -> takes two points in a space
No neg dist
symmetic
triangle inequality typically holds
d(x, y) <= d(x, z) + d(z, y)

Manhattan Dist:
Can calculate their similarity via their dist d based on the absolute diff of their cartesian cordi

Cosine similarity:
can calculate similarity through their vector cosine.
(cosine of the angle between two vectors)

-------------------------------------------------------------------------

Maximum similarity or minimum dist
d(x, y) = min(d(x, z) | z <- Y)

1-NN : the cloest training instance
K-NN : k nearest training instances

weighted k-nn : Classify the test input according to the
weighted accumulative class of the K nearest training instances,
where weights are based on similarity of the input to each of the
K neighbours.

[offset-weighted K-NN]: Classify the test input according to
the weighted accumulative class of the K nearest training
instances (sum of k nearest training instance), where weights are based on similarity of the input to
each of the K neighbours, factoring in an offset for the prior
expectation of a test input being a member of that class

-------------------------------------------------------------------------

Weighting Strategies:

-> Give each neighbour equal weight

-> weight the vote of each instance by its inverse linear distance from the test instance:
      
wj = dk - dj / dk - d1 if dj != d1
1 if dj (the one looking for) = d1
d1 is the closest and dk is the furthest neighbors.

wj = 1 / dj + epsilon


Breaking ties:
Equal num of votes for a given class:
       random tie breaking
       take class with highest prior prob
       see if the addition of the k+1th instace breaks the tie

smaller vals of k -> lower perfo due to noise (overfitting)
Larger k => zero-R perfo

*zero-R -> 

-> trial and error

k is generally set to an odd val

Implementation involves brute force computation of distances between a test instance and every training instance.

N instances
D dimensions

-> O(DN)

AS N grows. brute force becomes infeasible.

Why slow -> the model built by naive bayes/decision trees is generally much smaller than the dataset:
       predicting the class of a test instance requires approx O(CD) calc for naive bayers,
       and O(D) node traversals for a decision tree, given C lasses and D attributes.
       
Model built by k-NN is the dataset itself:
       - k-nn is lazy
       - the time we save in training is lost if we have to make many predictions


-------------------------------------------------------------------------

Strengths and Weaknesses of NN methods
Strengths
       - Simple
       - can handle arbitrarily many classes
       - Incremental
Weaknesses
       - need a useful distance func
       - need an averaging function for combining the labels of multiple training examples
       - Expensive (in terms of index accesses)
       - Everything is done at run time (lazy learner)
       - Prone to bias
       - Arbitrary K value

-------------------------------------------------------------------------

Nearest prototype classification:
A parametric variant of nearest neighbour classification is the nearest prototype, whereby
we calculate the cetroid of each class, and 
classify each test instance according to the class of the centroid is nearest to
Get the average and find the class with the closest average val.

Centroid is calculated by averaging the numeric vals along each axis:
for a class Cj = {}
proto Pj = <>
where each ak* = sigma ak / M

-------------------------------------------------------------------------

Support Vector Machine:
-> non probablistic binary linear classifier.
       - linear classifier for a two-class classification problem
       - The particular hyperplane it selects is the maximum margin hyperplane
       - soft margins allow some data points to violate the separating hyperplane
       - kernel func can be used to allow the SVM to find a non-linear separating boundary
       between two classes.

Goal is to find a hyperplane that separates two classes.

Linearly separable:

Not linearly separable:
if cannot draw one line to separate.

the hyperplane equation is:
w1x1 + w2x2...wmxm + b = 0
w · x + b = 0

• A linear classifier takes the form f (x) = wT x + b
• In 3D, this is a plane:

For a k-NN classifier it was necessary
to ’carry’ the training data.
-> take the training data every time performs K-NN.

For a linear classifier, the training
data is used to learn w (the “weight
vector”) and then (mostly) discarded.

If a point is far from the devider, then it's hard to predict.

It's difficult to rate the boundary decision.

For a given training set, would like to find a decision boundary
that allows us to make all correct and confident (far from the
decision boundary) predictions on the training examples.

SVM finds an optimal one.
maximises the dist betw' the hyperplane and the difficult points close to decision boundary.

Intuition: if there are no points near the decision surface, then there are no very uncertain
classification decisions.

Maximum margin solution: most stable under perturbations of
the inputs

Possibly large margin solution is
better even though one
constraint is violated


Trade-off between the margin
and the number of mistakes on
the training data

SVM-based classification:
Associate one class as positive (+1) and one as negative (-1)
-> pos area and neg area.

Find the best sol, which maximise the margin bet' the positive and negative training instance
(the model)

To make a prediction for a test instance t = t1, t2, ... tn

The value of f (t) can be transformed into a “probability”,
with some extra work

For small training sets, we can use a naive training method:
• Pick a plane w and b
• Find the worst classified sample yi
(Note: This step is computationally expensive for large data sets)
• Move plane w and/or b to improve the classification of yi
• Repeat steps 2-3 until the algorithm converges

-> takes too much time

To obtain a non-linear classifier, can transform data by applying a mapping func,
and then apply a linear classifier to the new feature vectors.

Kernel: 
make non separable problem separable
map data into better representational space

-------------------------------------------------------------------------

wTx + b = 0,

Objective is to find the data points that act as the boundaries of the two classes.

These are referred to as the suppor vects.
-> constrain the margin bet' the two classes.

-------------------------------------------------------------------------

Optimisation: Maximizing the margin
Want to choose w so that the margin 2 / |w| is maximised

Given the relo' between the margin, and the normalisation factor of the weight vector,
maximizing the margin corresponds to minimising ||w||.

Determination of model parameters corresponds to a convex
quadratic optimisation problem. Any local solution is also a
global optimum

When the two classes are not (completely) linearly separable:
-> introduce slack variables
{ξk } N k=1
and allow few points to be on the wrong side of the
hyperplane at some cost. The modified objective function:

Solving constrained optimisation problems uses the method of Lagrange multipliers.
that means a Lagrange multiplier αk for every
instance in the training set.

• Most αk are 0; the non-zero values correspond to support
vectors.

• If we wish to recover w and b, we can do so by only
considering the instances with non-zero αk.

• Effectively, we can ignore every training instance not on the
decision boundary at this point.

f (t) = Sigma αyxTt + b

If we need a non-linear SVM, we replace our dot product with
the corresponding kernel function:

f (t) = Sigma αyK(x, t) + b

-------------------------------------------------------------------------

SVMs are inherently two-class classifiers.
Most common approaches to extending to multiple classes:

       • one-versus-all (or one-versus-rest) classification
       choose class which classifies test data point with greatest
       margin
       • one-versus-one classification (one classifier per pair of
       classes)
       choose class selected by most classifiers
       
Training time becomes a serious issue, because we need to build
many SVMs...

if there are too many SVMs, it takes too much time to process.

Summary:
• SVMs is a high-accuracy margin classifier
• Learning a model means finding the best separating
hyperplane.
• Classification is built on projection of a point onto a
hyperplane normal.
• SVMs have lots of parameters that need to be optimised
(slow?).
• SVMs can be applied to non-linearly-separable data with an
appropriate kernel function.

Kernel function takes data as input and transform it into the required form.

-------------------------------------------------------------------------

If only nominal attributes:
       NB, 1-R, DT
       Need to alter the data into a format suitable for k-NN, NP, SVM
For k-NN and NP, can fall back to Hamming Distance:
       dH(A, B) = sigma o if ai = bi or 1 
But constructing a prototype is not well defined.

Randomly assign numbers to attribute values:
        if scale is constant between attributes -> not as bad as it sounds
        worse with higher-arity attributes (more attribute values)
        imposes an attribute ordering which may not exist.
        
ML Sol -> One hot encoding:
       If nominal att takes m vals, replace it with m Boolean Attributes
       Example:
              hot = [1, 0, 0]
              mild = [0, 1, 0]
              cool = [0, 0, 1]
             
  -> solves the problem of nominal attributes by massively increasing the feature space.
  
Nominal data is categorical data that assigns numerical 
values as an attribute to an object, animal, person or any other non-number. 
Remember, nominal data are numbers given as codes to certain objects. 
They are used to identify the objects only; they cannot be manipulated as numbers.

3 y
4 y
7 y
8 y
5 y
P(a=5|y) = 1/5
-> is not effective
-------------------------------------------------------------------------

Types of Naive Bayes:
       Multivariate NB: attributes are nominal, and can take any number of values
       Binomal NB: attributes are binary
       Multinomal NB: attributes are natural numbers
       Gaussian NB: attributes are real numbers
              instead of a probability mass func -> use a density func
              
Decision Trees:
       To build a decision tree, we label a node with an attribute, and branches with
       corresponding attribute values.
       Example -> outlook -> sunny, o'cast, rainy
       
If the attribute is numerical -> binarisation:

       Each node is labbeled with ak, and has two branches: one branch is ak <= m,
       the other is ak > m -> split into two
       Info Gain/ Gain Ration must be calculated for each non-trivial split point for each
       attribute
              naively, this each unique value is the dataset
              faster implementations constrain the number of split points
        Otherwise equ to ID3
              Binarisation leads to arbitrarily large trees.
              
        Sometimes it's better with ranges.
        
Discretisation -> translation of continuous atts onto nominal atts:

Discretisation is generally performed as a two-step process:
       decide how many values to map the feature on to
       
       map each continutous value onto a discrete value.
       
 Advantages:
       simple to implement
 Disadv:
       loss of generality
       no sense of numeric proximity/ordering
       describes the training data but nothing more (overfitting)

Partition values into bins of equal size:
Procedure 1: Identify the upper and lower bounds and partition the overall space into n equal intervals
 = equal width:
 
 Equal freq discretisation
 Adv: simple
 Disadb:
 arbitrary n
 
 Procedure 2: sort the values, and identify breakpoints which
produce n (roughly) equal-sized partitions = equal
frequency:

-------------------------------------------------------------------------

K-means Clustering
Given k, the k-means algo' is implemented in four steps:
1, Select k points at random to act as seed clusters
2, Assign each instance to the cluster with the nearest centroid
3, Compute seed points as the centroids of the clusters of the current partition
4, Go back to 2, stop when the assignment of instances to clusters is stable


One typical improvement runs k-means multiple times (with
random seeds), looking for a common clustering
• we can simply ignore runs which don’t converge within τ
iterations

Strengths:
• can handle numerical values.

Weaknesses:
• need multiple runs to have any certainty about convergence
• random behaviour
• sensitive to outliers
• still need to know number of intervals (k)

Naive Supervised Discretisation:
Sort the values and identify breakpoints in class membershi


64    65    68 69 70      70 
yes | no |  yes yes yes | no

Reposition any breadk points where there is no change in numeric value:

64    65    68 69 70   70 
yes | no |  yes yes yes no |

Set the breakpoints midway between the neighbouring values:
Adv: simple to implement
Disadv: usually creates too many categories (overfitting)

-> to avoid overfitting:
Delayt inserting a breakpoint until each cluster contains at least n instances of 
a single class:

64 65 68 69 | 70 71 72 72 75 ...
yes no yesyes| yes no no yes yes ..

-> merge neighbouring clusters until they reach a certain size

-------------------------------------------------------------------------

Probability Mass Func:
To discrete events:
P(temp = hot | y) = 0.25
Summing over every event gives 1: sigma P = 1
For numerical atts, assign probs to ranges
p(a <- [2.05, 3.00] | Y) = 0.38
Integral over some function:
0.38

Prob density func:
based on the gaussian disto (normal disto):
Given the mean and standard deviation of a disto X, it is possible to estimate the 
prob density for an obs X = x

Some relevant prop of gaussian
Symmetric about the mean
area under the curve = 1

Mean of a sample X is the average value it takes
Standard dev -> sqrt(sigma (val - mean)^2)

-------------------------------------------------------------------------

Better Models:
       Better perfo according to some evaluation metric

Side-goal:
       Seeing important features can suggest other important features
       Tell us interesting things about the problem
Side-goal:
       fewer features -> smaller models -> faster answer
              More accurate anwer >> faster answer.
              
Wrapper methods:
       Choose subset of attributes that give best perfo on the development 
       (with respect to a single learner)
       
For the weather data set:
       train model on outlook
       .. temperature
       
       ... outlook, temperature
       
      best perfor on data set -> best feature set
      
      adv: feature et with optimal perfo on development data
      disadv: takes a long time
 
Train and eval model on each single attribute
Choose best att

Until convergence:
       Train and eval mdoel on best atts, plus each remaining single attribute
       choose best attribute out of the remaining set
       
Iterate until perfo (accuracy) stops increasing
Takes 1/2 m^2 cycles for m attributes

In practice, converges much more quickly than this

Convegences to a sub optimal sol

Ablation approach:
       Start with all atttributes
       remove one att, train and eval model
       until divergence:
              from remaining atts, remove each att, train and evaluate model
       remove att that causes least perfo degredation
       
       Termination cond usually: perfo (accuracy) starts to degrade by more than epsilon
       Good
       -> removes irrelevant atts
       
       Bad
       -> assumes independence of atts
       -> take m^2 time; cycles are slower with more atts
       -> not feasible on non-trivial data sets.
       
       Some models eprform feature selection as part of the algo'
              Most notably, linear classifiers
              To some degree: SVMs and Logistic Regression
              To some degree: decision trees.
              
       Possible but difficult to control for inter-dependence of attributes
       
       
  What makes a single feature good?
       Better Models!
       Well correlated with class
       
 Independence:
       P(C|A) = P(C) given A doesn't make any changes.
       P(A,C) = P(A) P(C)
       
 We want atts that are not independent from class.
 
 P(A, C)/P(A)P(C)
= 1

If LHS >> 1, attribute and class occur together much more
often than randomly.

If LHS ∼ 1, attribute and class occur together as often as
we would expect from random chance

(If LHS << 1, attribute and class are negatively correlated.
More on this later.)

PMI(A = a, C = c) = log2 P(a, c) / P(a)P(c)

Attributes with greatest PMI: best atts (most correlated with class)

(Various notational shorthand conventions, like A → A = Y ,
where Y is the “interesting” value of a binary attribute)

Well correlated with [interesting] class
• Knowing a lets us predict c with more confidence]

Reverse correlated with class
• Knowing a¯ lets us predict c with more confidence

Well correlated (or reverse correlated) with uninteresting
class
• Knowing a lets us predict c¯ with more confidence
• Usually not quite as good, but still useful

Contigency tables: compact representation of these frequency
counts

MI(A, C) = sum of the point wise mutual info of all pairs.
A Yes
A NO
C Yes
C No

Combinations of these.

Chi-square
Similar but diff sol:

PMI, MI X^2

Check the val we actually observed O(W) with the expected value E(W):
       if the observed val is much greater than the expected val, a occurs more often with c than
       we would expect at random - predictive
       
       If O(W) is much lesser than the expected, a occurs less often with c than we would expect at random 
       
       If close, a occurs as often with c as we would expect randomly
       
       Similarly with X, Y, Z
       sigma sigma (O - E)^2 / E
       
       if the expected and observed are the same, then x^2 becomes 0
       
Chi-square can be used as normal, with 6 observed/expected
values.
• To control for score inflation, we need to consider “number
of degrees of freedom”, and then use the significance test
explicitly (beyond the scope of this subject)

Ordinal attributes (e.g. low, med, high or 1,2,3,4).
Three possibilities, roughly in order of popularity:
• Treat as binary
• Particularly appropriate for frequency counts where events
are low-frequency (e.g. words in short documents)
• Treat as continuous
• The fact that we haven’t seen any intermediate values is
usually not important
• Does have all of the technical downsides of continuous
attributes, however
• Treat as nominal (i.e. throw away ordering)


What makes a single feature good?
• Highly correlated with class
• Highly reverse correlated with class
• Highly correlated (or reverse correlated) with not class
... What if there are many classes?

Mutual Information is biased toward common, uninformative
features
• All probabilities: no notion of the raw frequency of events
• For example: 10% of the instances, a common attribute
occurs with a particular class, but 11% of instances are truly
of that class. Is this meaningful?
• Best features in a typical dataset might only have MI of
about 0.03 bits; 100th best for a given class, perhaps 0.001
bits
• Many very common features will be selected

Chi-square is biased toward rare, “informative” features
• This happens because of squaring the difference (rare means
small E)
• If a feature is seen rarely, but always with a given class, it
will be seen as “good”
• For example: a particular attribute is present in 100 out of
200K instances, but always with (a relatively rare) class. Is
this meaningful?
• Various “humps” where infrequent (1 instance, 2 instances,
3 instances, etc.) attributes are tied in the ranking
-------------------------------------------------------------------------

-------------------------------------------------------------------------

-------------------------------------------------------------------------



-------------------------------------------------------------------------

-------------------------------------------------------------------------


