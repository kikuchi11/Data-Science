Week1

Supervised (labeled) vs Unsupervised (unlabled)

Supervised Learner
Train Data -> Learner

Test Data & Learner -> Model

Test Data & Model -> Evaluation

Evaluation (Supervised):

Pick an evaluation metric comparing label vs prediction

Metrics:
Accuracy, Contingency Table (Correlation), Precision-Recall (# of True Positives), ROC curves (curve of probability)

Accuracy = True Positive/Actual Results or True Positive/(True Positive + False Positive)
True Positive & False Positive: when the actual result and predicted value match


When data is poor, cross-validate

Knowledge Technologies (matching data, databases) vs Machine Learning (predicting based on models, AI)

3 models
Linear

P(y|x) = probability of y given x

P(x,y) = probability of having (Knowledge Tech = x, ML = y)

Probability

Fi = null

Axioms:
p(f) >= for every even f in F

p(omega) = 1

Discrete vs Continuous:

Discrete:

Bernoulli

Binomial

Multinomial

Poisson


Continuous:

Uniform

Normal

Laplace

Gamma

Beta

Dirichlet


