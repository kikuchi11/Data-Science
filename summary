Week1

Supervised (labeled) vs Unsupervised (unlabled)

Supervised Learner
Train Data -> Learner

Test Data & Learner -> Model

Test Data & Model -> Evaluation

Evaluation (Supervised):

Pick an evaluation metric comparing label vs prediction

Metrics:
Accuracy, Contingency Table (Correlation), Precision-Recall (# of True Positives), ROC curves (curve of probability)

Accuracy = True Positive/Actual Results or True Positive/(True Positive + False Positive)
True Positive & False Positive: when the actual result and predicted value match


When data is poor, cross-validate

Knowledge Technologies (matching data, databases) vs Machine Learning (predicting based on models, AI)

3 models
Linear

P(y|x) = probability of y given x

P(x,y) = probability of having (Knowledge Tech = x, ML = y)

Probability

Fi = null

Axioms:
p(f) >= for every even f in F

p(omega) = 1

Discrete vs Continuous:

Discrete (Described by probability mass function):

Bernoulli : success or failure based on probability density function

Based on probability mass function:

Binomial

Multinomial

Poisson


Continuous (Described by probability density function):

Uniform

Normal (Probability distribution over all the real numbers, described by a mean and a variance) 

Laplace 

Gamma

Beta

Dirichlet (Sampling from the space of real numbers. Instead it is sampling over a probability simplex)
Probability Simplex : a bunch of numbers that add up to 1


