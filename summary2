What's classifier comb?
combining results obtained from a set of classifiers to achieve higher performance than each single classifier.

Given a number of classifiers, each classifies a same input x into a class label, and the labels maybe different for different classifiers. 
We seek a rule M(x) that combines these classifiers as a new one that performs better than anyone of them

What's meta classifier?
A classifier, which is usually a proxy to the main classifier, used to provide additional data preprocessing. 

Why is meta classifier good?
1,  the combination of lots of weak classifiers can
be at least as good as one strong classifier

2, the combination of a selection of strong
classifiers is (usually) at least as good as the best of the
base classifiers

What's base classier?
Component of classifier comb
has a high model variance

-------------------------------------------------------------

What's the error rate of classifier comb?
sigma e^i (1-e)^n-i
i = 13 till 25

i is 13 because 13 classifiers are more than half of the total.

What's logit?
the logarithm of the odds p/(1 − p) where p is the probability
It is a type of function that creates a map of probability values from to


What's 

-------------------------------------------------------------


What's 


What's 


What's 


What's 


What's 


What's 


What's

-------------------------------------------------------------



What's 


What's 


What's 


What's 
-------------------------------------------------------------

Is clustering supervised?
no. it's unsupervised.
no explicit or
implicit definition of class


What's K-means clustering?
select random k points or seeds and put instances into the closest centroid group
update the centroids as the means of the values assigned to each group
keep doing until it stops changing

What's soft k-means clustering?
soft (probablistic)

-------------------------------------------------------------

What's finite mixture?
mixed distribution with k component distributions
used to model the distribution (range) of attribute–value pairs in each cluster

What's Expected Maximisation algo?
estimation method with guaranteed “positive” hill-climbing characteristics relative to
the gradient of log-likelihood

Used to estimate (hidden) parameter values or cluster
membership or find the centroids in other words

How is EM done?

calculate the expected log-likelihood (= E(xpectation)
step)

compute the new parameter distribution Θn+1 from Θn,
that maximises the log-likelihood (= M(aximisation)
step)
estimate the values of missing values based on features with
known values
• estimate the component distributions of two loaded dice
from a sample set of their sum over N rolls

What's log likelihood?
gives us an estimate of the “goodness” of the cluster
model, and is guaranteed to increase on each iteration of
the algorithm

Convergence can be measured by the relative difference in
log likelihood from one iteration to the next; once this falls
below a certain predefined level , we can consider the
estimate to have converged

What are the adv and disadv of EM?
Advantages:
• guaranteed “positive” hill climbing behaviour
• fast to converge
• results in probabilistic cluster assignment
• (relatively) simple but powerful method

Disadvantages:
• possibility of getting stuck in a local maximum
• still rely on arbitrary k (but ...)
• tends to overfit data if “over-trained”

What helps us compare them to work out if one cluster analysis is “better” than another?
• test data eresults
• subjective evaluation
• similarity between clusters over multiple iterations

What are the applications of the clustering evaluation?
determining the optimal number of clusters for a given
dataset

evaluating how well the analysis fits the data

comparing clustering algorithms

hyperparameter tuning of a given clustering algorithm -> # of clusters

-------------------------------------------------------------

Are loglikelihood and naive bayes the same?
Yes

Is small difference in centroids between iterations better?
Yes -> better clustering analysis method

What are the two type os clustering?
Unsupervised: how cohesive are individual clusters/how
separate is one cluster from other clusters?

Supervised: how well do cluster labels match externally
supplied class labels?

What's a good clustering analysis?
have one or both of:

high cluster cohesion, i.e. instances in a given cluster
should be closely related to each other
1/sigma proximity

high cluster separation, i.e. instances in different clusters
should be distinct from each other
sigma proximity

How to determine β for regressions?
Gradient Descent!

How to build a logistic model?
Choose the best β, which correctly labels as positive or negative, as many test instances as possible.
-> Maximise log likelihood (naive bayes)

What relationship does linear regression capture?
a relationship between two variables or
attributes.
expressed as y = f (x)
y = β0 + β1x1 + ... + βDxD

How to fit the linear regression model?
Operationally, the line that minimises the distance between
all points and the line.
Euclidean Distance or mean square which squares the diff

-------------------------------------------------------------
*linear regression
continuous atts -> continuous classes
-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------





