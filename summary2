What's classifier comb?
combining results obtained from a set of classifiers to achieve higher performance than each single classifier.

Given a number of classifiers, each classifies a same input x into a class label, and the labels maybe different for different classifiers. 
We seek a rule M(x) that combines these classifiers as a new one that performs better than anyone of them

What's meta classifier?
A classifier, which is usually a proxy to the main classifier, used to provide additional data preprocessing. 

Why is meta classifier good?
1,  the combination of lots of weak classifiers can
be at least as good as one strong classifier

2, the combination of a selection of strong
classifiers is (usually) at least as good as the best of the
base classifiers

What's base classier?
Component of classifier comb
has a high model variance

-------------------------------------------------------------

What's the error rate of classifier comb?
sigma e^i (1-e)^n-i
i = 13 till 25

i is 13 because 13 classifiers are more than half of the total.

What's logit?
the logarithm of the odds p/(1 − p) where p is the probability
It is a type of function that creates a map of probability values


What's Gradient Descent?
optimization algorithm used to minimize some function by iteratively moving in the direction 
of steepest descent as defined by the negative of the gradient. 



-------------------------------------------------------------


What's active learning?
The main hypothesis in active learning is that if a learning algorithm can choose the data it wants to learn from, 
it can perform better than traditional methods with substantially less data for training.

What's passive learning?
Gathering a large amount of data randomly sampled from the underlying distribution 
and using this large dataset to train a model that can perform some sort of prediction. 

One of the more time-consuming tasks in passive learning is collecting labelled data. 
In many settings, there can be limiting factors that hamper gathering large amounts of labelled data.

What's least confidence?
in this strategy, the learner selects the instance for which it has the least confidence in its most likely label. 
Looking at the table, the leaner is pretty confident about the label for d1, 
since it thinks it should be labelled A with probability 0.9, however, 
it is less sure about the label of d2 since its probabilities are more spread and 
it thinks that it should be labelled B with a probability of only 0.5. Thus, using least confidence, 
the learner would select d2 to query it's actual label.
-> should learn more about d2 because it's less confident but only look at the highest prob label of each instance

What's Margin Sampling?
the shortcoming of the LC strategy, is that it only takes into consideration the most probable label and 
disregards the other label probabilities. The margin sampling strategy seeks to overcome this disadvantage by 
selecting the instance that has the smallest difference between the first and second most probable labels. 
Looking at d1, the difference (probability) between its first and second most probable labels is 0.81 (0.9 - 0.09) 
and for d2 it is 0.2 (0.5 - 0.3). Hence, the leaner will select d2 again.

highest minus lowest
*Each instance has multiple labels

What's entropy sampling?
To utilize all the possible label probabilities/
The entropy formula is applied to each instance and the instance with the largest value is queried. 
Using our example, d1 has a value of 0.155 while d2's value is 0.447 and so the learner will select d2 once again.


What's 


What's 


What's

-------------------------------------------------------------



What's 


What's 


What's 


What's 
-------------------------------------------------------------

Is clustering supervised?
no. it's unsupervised.
no explicit or
implicit definition of class


What's K-means clustering?
select random k points or seeds and put instances into the closest centroid group
update the centroids as the means of the values assigned to each group
keep doing until it stops changing

What's soft k-means clustering?
soft (probablistic)

-------------------------------------------------------------

What's finite mixture?
mixed distribution with k component distributions
used to model the distribution (range) of attribute–value pairs in each cluster

What's Expected Maximisation algo?
estimation method with guaranteed “positive” hill-climbing characteristics relative to
the gradient of log-likelihood

Used to estimate (hidden) parameter values or cluster
membership or find the centroids in other words

How is EM done?

calculate the expected log-likelihood (= E(xpectation)
step)

compute the new parameter distribution Θn+1 from Θn,
that maximises the log-likelihood (= M(aximisation)
step)
estimate the values of missing values based on features with
known values
• estimate the component distributions of two loaded dice
from a sample set of their sum over N rolls

What's log likelihood?
gives us an estimate of the “goodness” of the cluster
model, and is guaranteed to increase on each iteration of
the algorithm

Convergence can be measured by the relative difference in
log likelihood from one iteration to the next; once this falls
below a certain predefined level , we can consider the
estimate to have converged

What are the adv and disadv of EM?
Advantages:
• guaranteed “positive” hill climbing behaviour
• fast to converge
• results in probabilistic cluster assignment
• (relatively) simple but powerful method

Disadvantages:
• possibility of getting stuck in a local maximum
• still rely on arbitrary k (but ...)
• tends to overfit data if “over-trained”

What helps us compare them to work out if one cluster analysis is “better” than another?
• test data eresults
• subjective evaluation
• similarity between clusters over multiple iterations

What are the applications of the clustering evaluation?
determining the optimal number of clusters for a given
dataset

evaluating how well the analysis fits the data

comparing clustering algorithms

hyperparameter tuning of a given clustering algorithm -> # of clusters

-------------------------------------------------------------

Are loglikelihood and naive bayes the same?
Yes

Is small difference in centroids between iterations better?
Yes -> better clustering analysis method

What are the two type os clustering?
Unsupervised: how cohesive are individual clusters/how
separate is one cluster from other clusters?

Supervised: how well do cluster labels match externally
supplied class labels?

What's a good clustering analysis?
have one or both of:

high cluster cohesion, i.e. instances in a given cluster
should be closely related to each other
1/sigma proximity

high cluster separation, i.e. instances in different clusters
should be distinct from each other
sigma proximity

How to determine β for regressions?
Gradient Descent!

How to build a logistic model?
Choose the best β, which correctly labels as positive or negative, as many test instances as possible.
-> Maximise log likelihood (naive bayes)

What relationship does linear regression capture?
a relationship between two variables or
attributes.
expressed as y = f (x)
y = β0 + β1x1 + ... + βDxD

How to fit the linear regression model?
Operationally, the line that minimises the distance between
all points and the line.
Euclidean Distance or mean square which squares the diff

-------------------------------------------------------------
*linear regression
continuous atts -> continuous classes
-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------

-------------------------------------------------------------





