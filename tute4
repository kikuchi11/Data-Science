
1. For the following dataset:

(a) Classify the test instances according to the method of Nearest Prototype.

Find a centroid (prototype) for each class by averaging (taking the mean) of the attribute
values, for the instances of that class in the training data: because want to take the mean value for each label
for each class

for each label (apple, ibm, lemon, sun)
NP: Pf(class = fruit) = ⟨4+5 /2, 0+0 /2, 1+5 /2, 1+2 /2⟩
= ⟨4.5, 0, 3, 1.5⟩

Pc(class = computer) = ⟨2+1 /2, 5+2 /2, 0+1 /2, 0+7 /2⟩
=⟨1.5, 3.5, 0.5, 3.5⟩

We then classify a test instance according to the closest prototype. We will need to 
choose a similarity/distance function to do this; 
Use euclidean here.  Classify the test instance based on the distance.

dE(A,B) = sqrt(sigma (ak −bk)^2)

For the first test instance:
test instance label val - training instance mean val 
dE(T1,Pf) = √(2−4.5)2 +(0−0)2 +(3−3)1 +(1−1.5)2 =√6.5
dE(T1,Pc) = √(2−1.5)2 +(0−3.5)2 +(3−0.5)1 +(1−3.5)2 =√25

The FRUIT prototype is closer, so we will predict that the class of this instance is FRUIT.
For the second test instance:
dE(T2,Pf) = √(1−4.5)2 +(2−0)2 +(1−3)1 +(0−1.5)2 =√22.5
dE(T2,Pc) = √(1−1.5)2 +(2−3.5)2 +(1−0.5)1 +(0−3.5)2 =√15

• This time, the COMPUTER prototype is closer, so we will predict that the class of this
instance is COMPUTER.

(b) Using the Euclidean distance measure, classify the test instances using the 1-NN method.

*This is similar to the previous question, but instead of calculating the distance 
between a test instance and a prototype, we will calculate the distance between the test instance
and each training instance:

1-nn find the closest.

dE(T1,A) = √(2−4)2 +(0−0)2 +(3−1)2 +(1−1)2 = √8

dE(T1,B) = √(2−5)2 +(0−0)2 +(3−5)2 +(1−2)2 = √14

dE(T1,C) = √(2−2)2 +(0−5)2 +(3−0)2 +(1−0)2 = √35

dE(T1,D) =  √(2-1)2+(0−2)2 +(3−1)2 +(1−7)2 = √45

So no averaging by classes but compare instance themselves.

The nearest neighbour is the one with the smallest distance — here, this is instance A, which is a FRUIT instance. Therefore, we will classify this instance as FRUIT.
The second test instance is similar:

Answer depends on distance method

(c) Using the Manhattan distance measure, classify the test instances using the 3-NN method,
for the three weightings we discussed in the lectures: majority class, inverse distance, inverse
linear distance.

The first thing to do is to calculate the Manhattan distances, 
which is like the Euclidean distance, but without the squares/square root:
Without squaring, so the distance is not so important

dM(A,B) = sigma (k) |ak −bk|

dE(T1,A) = |2−4|+|0−0|+|3−1|+|1−1|=4 
dE(T1,B) = |2−5|+|0−0|+|3−5|+|1−2|=6 
dE(T1,C) = |2−2|+|0−5|+|3−0|+|1−0|=9 
dE(T1,D) = |2−1|+|0−2|+|3−1|+|1−7|=11 
dE(T2,A) = |1−4|+|2−0|+|1−1|+|0−1|=6 
dE(T2,B) = |1−5|+|2−0|+|1−5|+|0−2|=12 
dE(T2,C) = |1−2|+|2−5|+|1−0|+|0−0|=5 
dE(T2,D) = |1−1|+|2−2|+|1−1|+|0−7|=7

3-nn:


(d) Can we do weighted k-NN using cosine similarity?

 Ofcourse!If anything,this is easier than with a distance, because we can assign weighting 
 for each instance using the cosine similarity directly. 
 An overall weighting for a class can be obtained by 
 summing the cosine scores for the instances of the corresponding class, 
 from among the set of nearest neighbours.

2. Revise SVMs, particularly the notion of “linear separability”.
(a) If a dataset isn’t linearly separable, an SVM learner has two major options. What are they,
and why might we prefer one to the other?

Soft margins: we permit a few points to be on the “wrong” side of the line, at a penalty, 
if this allows us to find a better (wider) margin.


Kernel methods: we (effectively) transform the data into a higher-dimensional space. 
Conceptually, this is by creating new dimensions (and consequently, new attributes), 
but the “kernel trick” allows us to do this without making the transformation explicit 
(and so, saving us some computation time).

• If we suspect that the data is essentially linearly separable, except that a few points will be mis-classified 
(for example, if there are outliers in the data), then using a soft-margin SVM is the better choice. 
The kernel trick is too powerful to account for just a few instances 
(and, in the worst case, will generate a spurious transformation of the data to deal with them).

• If, on the other hand, we suspect that the data isn’t really linearly separable — 
because the instance topology isn’t linear, but polynomial or circular or something — 
then the soft margin SVM will produce a spurious margin, because there really isn’t a 
way to draw a straight line through the data at all. (This can be confirmed empirically, 
by cross- validating, at the cost of some time.)

• One major concern, separate to these notions is that the most popular method for building an SVM (SMO) 
converges much more quickly with a linear kernel than with a non-linear kernel. (This can be seen empirically.) 
Consequently, unless you have a lot of time to wait for the training cycles, 
non–linear kernels are generally only used as a last resort. 
(And many problems don’t really require them anyway!)

(b) Contrary to many geometric methods, SVMs work better (albeit slower) with large attribute
sets. Why might this be true?

The clearest way of seeing this is to imagine that the dataset has a number of useful attributes 
(as in, they are strongly or moderately predictive of the class), 
and a number of not-so-useful attributes (as in, they are marginally predictive, or not predictive at all).

• Most geometric methods calculate some kind of similarity or distance metric, which
assumes that every attribute is equally important. For example, using the Manhattan Distance, the difference between the values 
of the useful attributes can be small, but if the difference between the not-so-useful attributes is large, 
then the two instances will be not-so-similar (they will be far apart in space), as it considers both non-useful
and useful attributes.

• A typical SVM gets around this problem by (effectively) finding different 
“weights” (how important each attribute is to the target) for each attributes 
these form the normal vector w; the weights of the not-so-useful attributes will 
(hopefully) be negligible, compared to the weights of the useful attributes, 
so that the basis for the prediction (hopefully) becomes more meaningful.

• There are other issues too, like how the SVM is building a linear combination 
(or non- linear combination, if we are using a non-linear kernel) of the attributes, 
rather than treating them all as completely distinct.

3. We have now seen a decent selection of (supervised) learners:
• Naive Bayes
• 0-R
• 1-R
• Decision Trees
• k-Nearest Neighbour
• Nearest Prototype
• Support Vector Machines


(a) For each, identify the model built during training.
NB: this is a set of prior probabilities P(cj) and a set of posterior probabilities P(ak|cj) -> prior times
posterior

0R : majority class Y (doesn't matter what attributes there are)

1R: use the number of atts to find a class -> this is an attribute, and the majority class of 
the instances (a label) for each value -> find an attribute which is likedly to predict the class.

DT: this is the tree itself: every non-terminal node is labelled with an
attribute, and each branch with an attribute value; every leaf is labelled with a class -> it reaches a class
at the end (one with a high information gain (low entropy -> easy to predict) is often overestimated as they
stay in the top and have more power to influence)

k-NN: this is just the dataset itself! It doesn't do anything.  It compares the test instance and training instances.

NP: this is a prototype (vector, centroid, average of instances) for each class

SVM: this is the maximum-margin hyperplane, defined by w and b, or equivalently 
by a set of Lagrance multipliers αk, and the corresponding training instances for which αk > 0.

(b) Rank the learners (approximately) by how fast they can classify a large set of test instances.
(Note that this is largely independent of how fast they can build a model, and how well they
work in general!)

Talking about after models are built
C = class
D = # attributes
N = data

All of them are supervised.

0R: O(0)
1R: O(1)
DT: O(D) 
SVM: O(D) need to scan all at least once
NB: O(CD+C)
NP: O(DC)
K-nn: O(ND + k)

 Given N instances, C classes, and D attributes:
• The fastest few are clear: 0-R (O(0)), 1-R (O(1)), and Decision Trees (O(D)). 
• The next two are essentially equally fast: NP (O(CD)) and NB (O(CD + C)).

An SVM isn’t built to deal with multiple classes6. If there are exactly two classes, it
is just as fast as the previous two, which in turn are essentially 
indistinguishable from a Decision Tree. If there are many classes, 
and we are using a one-vs-all strategy for building our SVM(s), 
it is approximately the same as NB (O(CD + C)). If we are using one-vs-one, 
it is somewhat slower (O(C2D + C2)).

k-NN is much, much slower, because it must make a comparison for each instance (O(ND + k)).



