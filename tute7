1. Revise the difference between supervised and unsupervised machine learning.
Then, consider the following dataset:

i a i l s LABEL
A 4 0 1 1 FRUIT
B 5 0 5 2 FRUIT
C 2 5 0 0 COMP
D 1 2 1 7 COMP
E 2 0 3 1 ?
F 1 0 1 0 ?


In a supervised learning model, the algorithm learns on a labeled dataset, 
providing an answer key that the algorithm can use to evaluate its accuracy on training data. 
An unsupervised model, in contrast, provides unlabeled data that 
the algorithm tries to make sense of by extracting features and patterns on its own.


2. Treat the problem as an unsupervised machine learning problem (excluding the id and LABEL
attributes), and calculate the clusters according to (hard) k-means with k = 2, using the Manhattan
distance:
(a) Using seeds A and D.

(b) Using seeds A and F.


A: 4, 0, 1, 1
F: 1, 0, 1, 0

Once classify instances into either one of these centroids based on distance, recalculate the centroids 
by taking the mean vals of instances in each of the centroids. Repeat these steps until there's no change
in the assignment of instances.

3. Repeat the previous question using “soft” k-means, with a “stiffness” β = 1.
When B is small, can ignore the difference between cluster 1 & 2

Soft k means instead uses to update the centroids
z1 = e^- B * c1val / (e^ B * c1val + e^ B * c2val)
z2 = e^- B * c2val / (e^ B * c1val + e^ B * c2val)

Multiply each instance vals by their corresponding z1s and z2s, and then divide it by the sum of those instance vals


  c1    c2      z1      z2 
A 0     4       e^- B * 0 / (e^ B * -0 + e^ B * -4) = 9
B 6     10
C 9     7
D 11     9
E 4     4
F 4     0

4. What is logic behind the EM (expectation maximisation) algorithm, when used for clustering?
(a) Explain the significance of the “E” step, and the “M” step.

Expectation: randomly assign labels and build a loglikelihood model
Maximisation: use that model to assign labels again
And keep repeating these steps.

It's unsupervised learning so it cannot assign the correct labels.

5. What is semi–supervised learning, and when is it desirable?

semi-supervised learning algorithms are trained on a combination of labeled and unlabeled data. 
This is useful for a few reasons. First, the process of labeling massive amounts of data for 
supervised learning is often prohibitively time-consuming and expensive. What’s more, 
too much labeling can impose human biases on the model. That means including lots of 
unlabeled data during the training process actually tends to improve the accuracy of the 
final model while reducing the time and cost spent building it.

Used for webpage classification, speech recognition, or even for genetic sequencing.

(a) What is self training?
Self-Directed Training refers to the form of training where the learner 
takes responsibility for managing their own training, from the content 
they select to timing and delivery.

If you want to do Supervised Learning, you've to create a data-set to train. 
It's not always an easy task to create a huge data-set. 
So, here Reinforcement Learning comes into the picture, where an agent learns entirely by itself.

NB, SVM: supervised method

(b) What is the logic behind active learning, and what are some methods to choose instances for
the oracle?

The main hypothesis in active learning is that if a learning algorithm can choose the 
data it wants to learn from, it can perform better than traditional methods with 
substantially less data for training.

-> ignores instances it's not confident with.

Membership Query Synthesis: his is a big term which simply means that the learner generates/constructs 
an instance (from some underlying natural distribution). 
For example, if the data is pictures of digits, the learner would create an image that 
is similar to a digit (it might be rotated or some piece of the digit excluded) and 
this created image is sent to the oracle to label.

Stream-Based Selective Sampling: in this setting, you make the assumption that getting an 
unlabelled instance is free. Based on this assumption, you then select each unlabelled 
instance one at a time and allow the learner to determine whether it wants to query the 
label of the instance or reject it based on its informativeness. 

Select informative instances.

