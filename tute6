1. What is the Gradient Descent method, and why is it important?

Gradient Descent is a mechanism for finding the minimum of a (convex) multivariate function, where we can find its partial derivatives.
• This has numerous applications — most intuitively, in determining the regression weights
which minimise an error function over some training data set.

2. What is Regression? How is it similar to Classification, and how is it different?

• Our target attribute (class) is nominal in classification, but numeric (continuous) in Regression.
• Consequently, we can’t exhaustively assess the likelihood of each class.

(a) What is Linear Regression? In what circumstances is it desirable, and it what circumstances
is it undesirable?

• We attempt to build a linear model to predict the target values, by finding a weight for
each attribute. The prediction is therefore sigma weight attribute -> sum (weight * each attribute)

• Many (but not all) relationships can be approximately described by such a model

(b) How do we build a (linear) regression model? What is RSS and what advantages does it
have over (some) alternatives?

• By learning the weights using Gradient Descent, with respect to an error function.
• This assumes that our error function is convex (polynomial) — so that there is a unique solution. RSS
is one popular choice for this: where we attempt to minimise the sum of the squared
differences between our predicted values and the actual target values from the training
data.

3. Recall that the update rule for Gradient Descent with respect to RSS is as follows:

Build a Linear Regression model, using the following instances:

Recall that gradient descent:
– iteratively improves our estimates of the prediction line (according to parameters β)
– is based on the partial derivatives (which together define the gradient) of the error function (in this case, RSS)
– tries to find a weight (βk) for each attribute (k ∈ [1..D]), including a dummy attribute
numbered 0 (known as the “bias”)


• We need to specify an initial estimate for the weights — here, I will use yˆ = 0 + 0x. Recall
that because RSS is convex, the choice of initial estimate shouldn’t affect our overall answer.
Recall that gradient descent:
– iteratively improves our estimates of the prediction line (according to parameters β)
– is based on the partial derivatives (which together define the gradient) of the error function (in this case, RSS)
– tries to find a weight (βk) for each attribute (k ∈ [1..D]), including a dummy attribute
numbered 0 (known as the “bias”)

• We need to specify an initial estimate for the weights — here, I will use yˆ = 0 + 0x. Recall
that because RSS is convex, the choice of initial estimate shouldn’t affect our overall answer.


• We also need to choose the learning rate α; here we will say α = 0.05
• Skipping the workings, our update rule for each βk is based on the error of our prediction yˆ
for each instance (yi − yˆi):


4. What is Logistic Regression?

• We build a (linear) regression model, where the target is (close to) 1 for instances of the positive class, 
and (close to) 0 for instance of the negative class. (Suitably re-interpretted for multi-class problems.)

*Logistic regression is the appropriate regression analysis to conduct when the dependent variable is 
dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.
Linear regression.

*Linear reg is used when the dependent variable is continuous and nature of the regression line is linear/

(a) How is Logistic Regression similar to Naive Bayes and how is it different? 
In what circumstances would the former be preferable, and in what circumstances would the latter?

• Effectively, both methods are attempting to find the class c for a test instance T, by maximising P (c|T) -> likelihood.

• In Naive Bayes, we make some simplifying assumptions, most notably, that the attributes 
are conditionally independent of the class labels. -> p(attributes|class) should be the same as p(attributes), which doesn't usually 
happen in real life.

Two random variables (a,b) are independent, if
𝑝(𝑎,𝑏)=𝑝(𝑎)𝑝(𝑏)

or

𝑝(𝑎|𝑏)=𝑝(𝑎)
such as, championship win(class), weather(att), defense(att)

• In Logistic Regression, we attempt to model this directly, without the simplifying assumptions. 
This is possible because we don’t attempt to generate the class probabilities; 
we only attempt to discriminate amongst the various classes.

*linear regression can take values from -infinity to +infinity. But our class probabilities range from 0 to 1. 
Put simply/blunty a logistic regression model is not a classifier (gives a val but can't classify). 
It is a model for the probability parameter of the binomial distribution. This is why predict() gives probabilities.

In order to make it a classifier you need to specify a function converts probabilities into classes. 

(b) What is “logistic”? What are we “regressing”?
We typically apply the logistic function 1 /  1+e−m to the regression output (β · x), which has
an easy–to–calculate derivative, and has a range of [0, 1]

1 / 1+e^-x -> logistic function.

(c) How do we train a Logistic Regression model? In particular, what is the significance of the
following
argmax sigma yiloghβ(xi)+(1−yi)log(1−hβ(xi))

• This is a confusing way of stating that we want the output of the linear regression to be
(very) positive when the target class is 1, and (very) negative when the target class is 0. -> accurate
• We want to choose a set of parameters β which maximises this objective function; we use
Gradient Ascent to find them.

